{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4090f78",
   "metadata": {},
   "source": [
    "# Objects365 Dataset Download and Processing\n",
    "This notebook downloads and processes the Objects365 dataset, generating annotations in YOLO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9247375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from typing import List, Union, Tuple, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from zipfile import ZipFile, is_zipfile\n",
    "from itertools import repeat\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# Verify that pycocotools is installed\n",
    "try:\n",
    "    from pycocotools.coco import COCO\n",
    "except ImportError:\n",
    "    print(\"Installation of pycocotools required. Run: pip install pycocotools>=2.0\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869e8459",
   "metadata": {},
   "source": [
    "## Support Functions\n",
    "These functions handle downloading, extraction, and conversion of annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1763e709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_with_resume(url: str, dest: Union[str, Path], retry: int = 3) -> None:\n",
    "    \"\"\"\n",
    "    Downloads a file from a given URL with support for resuming partial downloads.\n",
    "    \"\"\"\n",
    "    headers = {}\n",
    "    if os.path.exists(dest):\n",
    "        headers['Range'] = f\"bytes={os.path.getsize(dest)}-\"\n",
    "    with requests.get(url, headers=headers, stream=True) as r:\n",
    "        total_size = int(r.headers.get('content-length', 0)) + os.path.getsize(dest)\n",
    "        if r.status_code == 416:\n",
    "            print(f\"{dest} is already fully downloaded.\")\n",
    "            return\n",
    "        elif r.status_code not in (200, 206):\n",
    "            raise Exception(f\"Failed to download {url}: {r.status_code}\")\n",
    "        with open(dest, \"ab\") as f, tqdm(\n",
    "            desc=f\"Downloading {Path(dest).name}\",\n",
    "            total=total_size,\n",
    "            unit=\"B\",\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024,\n",
    "            initial=os.path.getsize(dest),\n",
    "        ) as bar:\n",
    "            for chunk in r.iter_content(chunk_size=65536):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    bar.update(len(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e30b5135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyxy2xywhn(xyxy: np.ndarray, w: Union[int, float] = 640, h: Union[int, float] = 640, clip: bool = False, eps: float = 0.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Converts bounding box coordinates from (x_min, y_min, x_max, y_max) format \n",
    "    to normalized (x_center, y_center, width, height) format.\n",
    "    \"\"\"\n",
    "    if clip:\n",
    "        xyxy[:, 0] = np.maximum(0, np.minimum(xyxy[:, 0], w - eps))\n",
    "        xyxy[:, 1] = np.maximum(0, np.minimum(xyxy[:, 1], h - eps))\n",
    "        xyxy[:, 2] = np.maximum(0, np.minimum(xyxy[:, 2], w - eps))\n",
    "        xyxy[:, 3] = np.maximum(0, np.minimum(xyxy[:, 3], h - eps))\n",
    "    \n",
    "    y = xyxy.copy()\n",
    "    y[:, 0] = ((xyxy[:, 0] + xyxy[:, 2]) / 2) / w  # x center\n",
    "    y[:, 1] = ((xyxy[:, 1] + xyxy[:, 3]) / 2) / h  # y center\n",
    "    y[:, 2] = (xyxy[:, 2] - xyxy[:, 0]) / w        # width\n",
    "    y[:, 3] = (xyxy[:, 3] - xyxy[:, 1]) / h        # height\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb86d63",
   "metadata": {},
   "source": [
    "## Directory Setup\n",
    "Set the base directories for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bea43e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = 16  # Number of threads for downloading\n",
    "base_dir = Path(\"/mnt/e/objects365\")\n",
    "base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create directories\n",
    "for p in [\"images\", \"labels\"]:\n",
    "    for q in [\"train\", \"val\"]:\n",
    "        (base_dir / p / q).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24d8515",
   "metadata": {},
   "source": [
    "## Download Annotations and Images\n",
    "Download the necessary files for the Objects365 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23cc1630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train in 51 patches ...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/e/objects365/zhiyuan_objv2_train.tar.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Download annotations\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m split == \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[43mdownload_with_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43murl\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43mzhiyuan_objv2_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msplit\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.tar.gz\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mzhiyuan_objv2_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msplit\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.tar.gz\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m split == \u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     12\u001b[39m     download_with_resume(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mzhiyuan_objv2_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m\"\u001b[39m, base_dir / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mzhiyuan_objv2_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mdownload_with_resume\u001b[39m\u001b[34m(url, dest, retry)\u001b[39m\n\u001b[32m      7\u001b[39m     headers[\u001b[33m'\u001b[39m\u001b[33mRange\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbytes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.path.getsize(dest)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m requests.get(url, headers=headers, stream=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m r:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     total_size = \u001b[38;5;28mint\u001b[39m(r.headers.get(\u001b[33m'\u001b[39m\u001b[33mcontent-length\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0\u001b[39m)) + \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetsize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m r.status_code == \u001b[32m416\u001b[39m:\n\u001b[32m     11\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdest\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is already fully downloaded.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen genericpath>:62\u001b[39m, in \u001b[36mgetsize\u001b[39m\u001b[34m(filename)\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/mnt/e/objects365/zhiyuan_objv2_train.tar.gz'"
     ]
    }
   ],
   "source": [
    "for split, patches in [(\"train\", 50 + 1), (\"val\", 43 + 1)]:\n",
    "    print(f\"Processing {split} in {patches} patches ...\")\n",
    "    images, labels = base_dir / \"images\" / split, base_dir / \"labels\" / split\n",
    "\n",
    "    # Base URL\n",
    "    url = f\"https://dorc.ks3-cn-beijing.ksyun.com/data-set/2020Objects365%E6%95%B0%E6%8D%AE%E9%9B%86/{split}/\"\n",
    "\n",
    "    # Download annotations\n",
    "    if split == \"train\":\n",
    "        download_with_resume(f\"{url}zhiyuan_objv2_{split}.tar.gz\", base_dir / f\"zhiyuan_objv2_{split}.tar.gz\")\n",
    "    elif split == \"val\":\n",
    "        download_with_resume(f\"{url}zhiyuan_objv2_{split}.json\", base_dir / f\"zhiyuan_objv2_{split}.json\")\n",
    "\n",
    "    # Download images\n",
    "    if split == \"train\":\n",
    "        print(f\"Downloading training images ({patches} patches)...\")\n",
    "        for i in range(patches):\n",
    "            download_with_resume(f\"{url}patch{i}.tar.gz\", images / f\"patch{i}.tar.gz\")\n",
    "    elif split == \"val\":\n",
    "        print(\"Downloading validation images v1...\")\n",
    "        for i in range(15 + 1):\n",
    "            download_with_resume(f\"{url}images/v1/patch{i}.tar.gz\", images / f\"v1_patch{i}.tar.gz\")\n",
    "        print(\"Downloading validation images v2...\")\n",
    "        for i in range(16, patches):\n",
    "            download_with_resume(f\"{url}images/v2/patch{i}.tar.gz\", images / f\"v2_patch{i}.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f526119",
   "metadata": {},
   "source": [
    "## Annotation Processing\n",
    "Convert annotations to YOLO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167f106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [\"train\", \"val\"]:\n",
    "    annotations_path = base_dir / f\"zhiyuan_objv2_{split}.json\"\n",
    "    if annotations_path.exists():\n",
    "        coco = COCO(annotations_path)\n",
    "        names = [x[\"name\"] for x in coco.loadCats(coco.getCatIds())]\n",
    "\n",
    "        for cid, cat in enumerate(names):\n",
    "            catIds = coco.getCatIds(catNms=[cat])\n",
    "            imgIds = coco.getImgIds(catIds=catIds)\n",
    "\n",
    "            for im in tqdm(coco.loadImgs(imgIds), desc=f\"Class {cid + 1}/{len(names)} {cat}\"):\n",
    "                width, height = im[\"width\"], im[\"height\"]\n",
    "                path = Path(im[\"file_name\"])\n",
    "\n",
    "                try:\n",
    "                    label_file = labels / path.with_suffix(\".txt\").name\n",
    "                    with open(label_file, \"a\", encoding=\"utf-8\") as file:\n",
    "                        annIds = coco.getAnnIds(imgIds=im[\"id\"], catIds=catIds, iscrowd=None)\n",
    "                        for a in coco.loadAnns(annIds):\n",
    "                            x, y, w, h = a[\"bbox\"]\n",
    "                            xyxy = np.array([[x, y, x + w, y + h]])\n",
    "                            x, y, w, h = xyxy2xywhn(xyxy, w=width, h=height, clip=True)[0]\n",
    "                            file.write(f\"{cid} {x:.5f} {y:.5f} {w:.5f} {h:.5f}\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing image {path}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
